---
title: "Final Project models"
author: "Savannah Farney"
date: "4/24/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(tidyverse)
library(ggplot2)
library(base)
#music<- read.csv("genres_v2.csv", header = TRUE)
music<- read.csv("~/Github/STAT518Project/genres_v2.csv", header = TRUE) 

#count unique values
df<-sapply(lapply(music, unique), length)
df
#count nas
as.data.frame(colSums(is.na(music)))
as.data.frame(colSums(!is.na(music)))

music<-music[,-c(22,21,20,16,15,14,13, 12)]

music<-na.omit(music)
music$key <- as.factor(music$key)
music$mode <- as.factor(music$mode)
music$time_signature <- as.factor(music$time_signature)

```

## R Markdown

#MODELING TIME

```{r}
set.seed(1234)
#idx <- read.table('music.traintest')
#get random numbers for idx.tr
n<-nrow(music); n

idx.tr <- sample(n, .7*n) #idx$V1 == 0
music.tr <- music[idx.tr,]
music.tst <- music[-idx.tr,]

n<-nrow(music.tst); n

idx.tr <- sample(n, (2/3)*n) #idx$V1 == 0
music.t3 <- music.tst[-idx.tr,]
music.tst <- music.tst[idx.tr,]

summary(music)
ggplot(data=music, mapping = aes(x=genre)) +
  geom_bar()

summary(music.tr)
ggplot(data=music.tr, mapping = aes(x=genre)) +
  geom_bar()

summary(music.tst)
ggplot(data=music.tr, mapping = aes(x=genre)) +
  geom_bar()

summary(music.t3)
ggplot(data=music.tr, mapping = aes(x=genre)) +
  geom_bar()


```


##Model Selection

```{r}
library(leaps)
#Best subset

#forward selection

#backward selection

#others I have missed
```

##Logistical regression / multinomial regression
(https://it.unt.edu/sites/default/files/mlr_jds_aug2011.pdf)

I ended up deciding to implement this package. Code doesn't run with type included. 

```{r Logistical_Regression}
library(nnet)

genre <- as.factor(music.tr$genre)
multi <- multinom(genre~., data = music.tr)
#summary(multi) This command takes a really long time to run for some reason. 
table(as.factor(music.tst$genre),as.factor(predict(multi, newdata=music.tst)),dnn=c('TRUE', 'Logistic'))
round(table(as.factor(music.tst$genre),as.factor(predict(multi, newdata=music.tst)),dnn=c('TRUE', 'Logistic'))*100/nrow(music.tst),2)
```

```{r}
#figure out how to do this for more than one category
#library(mlogit)
#mdata<- mlogit.data(music, varying = NULL, choice="genre", shape = "wide")
#model.1 <- mlogit(genre~., data = mdata, reflevel = "1")
#do five times, look at largest


#QDA looking
#fit <- glmnet(x, y, family = "multinomial", type.multinomial = "grouped") plot(fit, xvar = "lambda", label = TRUE, type.coef = "2norm")
#cvfit <- cv.glmnet(x, y, family = "multinomial", type.multinomial = "grouped") plot(cvfit)
#predict(cvfit, newx = x[1:10,], s = "lambda.min", type = "class")
```

```{r}
#two categories only!!! Will not work!!!!
#lgr <- glm(genre~., data=music.tr, family = binomial)
#lgr.step <- step(lgr, trace=0)
#summary(lgr)
#summary(lgr.step)

#prob5 <-predict(lgr, newdata=music.tst, type='response')
#est.55 <- ifelse(prob5>0.5, 1, 0)
#table(music.tst$genre, est.55, dnn=c('TRUE', 'Logistic'))

#prob <- predict(lgr.step, newdata=music.tst, type='response')
#est.5 <- ifelse(prob>0.5, 1, 0)
#table(music.tst$genre, est.5, dnn=c('TRUE', 'Logistic'))
#mean(music.tst$genre != est.5)
```

##Maximum likelihood
  #Bayes theorem

##LDA
(https://multivariatestatsjl.readthedocs.io/en/latest/mclda.html)
```{r}
library(MASS)

lda <- lda(as.factor(genre)~., data=music.tr)
lda

table(music.tst$genre, predict(lda, newdata=music.tst)$class, dnn=c("True", "LDA"))
mean(music.tst$genre != predict(lda,  newdata=music.tst)$class)
```

##QDA
(https://rpubs.com/aaronsc32/qda-several-groups)
```{r}
#QDA is only designed for working with numerical predictors, so the three factor predictors must be omitted.
qda <- qda(genre~.-key-mode-time_signature, data=music.tr)
table(music.tst$genre, predict(qda, newdata=music.tst, type='response')$class, dnn=c("True", "QDA"))
mean(music.tst$genre != predict(qda,  newdata=music.tst)$class)
```

##Nearest Neighbor
```{r}
library(class)

knn.cls<-knn(train = music.tr[, 1:13], test = music.tst[, 1:13], music.tr[, 14], k=23)
table(music.tst$genre, knn.cls, dnn=c("True", "KNN K=23"))
mean(as.numeric(as.integer(factor(music.tst$genre))) != knn.cls)

knn.cls<-knn(train = music.tr[, 1:13], test = music.tst[, 1:13], music.tr[, 14], k=17)
table(music.tst$genre, knn.cls, dnn=c("True", "KNN K=17"))
mean(as.numeric(as.integer(factor(music.tst$genre))) != knn.cls)


knn.cls1<-knn(train = music.tr[, 1:13], test = music.tst[, 1:13], music.tr[, 14], k=1)
table(music.tst$genre, knn.cls1, dnn=c("True", "KNN K=1"))
mean(as.numeric(as.integer(factor(music.tst$genre))) != knn.cls1)

knn.cls5<-knn(train = music.tr[, 1:13], test = music.tst[, 1:13], music.tr[, 14], k=5)
table(music.tst$genre, knn.cls5, dnn=c("True", "KNN K=5"))
mean(as.numeric(as.integer(factor(music.tst$genre))) != knn.cls5)
```

##Naive Bayes

##Resampleing methods
  #validation set approach
  #leave-one-out Cross Validation
  #k-fold Cross-Validation
  #bootstrap

#Decision trees
##Classification Tree
```{r}
library(tree)
#full tree
trMusic <- tree(as.factor(genre)~., data=music, minsize = 1)
tree:::print.tree(trMusic)
plot(trMusic); text(trMusic, col='blue', pretty=TRUE, cex=.8)
#how many nodes?
cv.Music <- cv.tree(trMusic)
plot(cv.Music)
#prune tree
pr.tr <- prune.tree(trMusic, best=11)
plot(pr.tr); text(pr.tr, pretty=TRUE, col='red', cex=.8)
tree:::print.tree(pr.tr)
#look at doc to help get all of the genres
pb <- predict(pr.tr, newdata = music.tst, type = 'class')
table_mat <- table(music.tst$genre, pb)
table_mat
mean(music.tst$genre != pb)
```

##Random Forests
Random Forest (This does work now!)

```{r}
set.seed(2)
library(randomForest)
## Random Forest
rf.music<- randomForest(as.factor(genre)~., data=music.tr, mtry=4)
importance(rf.music)
varImpPlot(rf.music, cex = .65)
ranforest <- predict(rf.music, newdata = music.tst, type = 'class')
table_mat2 <- table(music.tst$genre, ranforest)
table_mat2
mean(music.tst$genre != ranforest)
#find better way to print?
```

##Bagging
Bagging
```{r Bagging}
set.seed(2)
## Bagging 
bag <- randomForest(as.factor(genre)~., data = music.tr, mtry=13, importance = TRUE, proximity = TRUE)
importance(bag)
varImpPlot(bag, cex=.65)
bagging <- predict(bag, newdata = music.tst, type = 'class')
table_mat <- table(music.tst$genre, bagging)
table_mat
```

#Out-Of-Bag Error?
using the observations that were not used in the bagging to test how the tree is doing

##Boosting
Boosting
```{r}
library(gbm)
library(adabag)
set.seed(2)
## Boosting for Classification
music.tr$genre<-as.factor(music.tr$genre)
boost <- boosting(genre~., data=music.tr, mfinal=20)
boost
boost$importance
# Compare the Cross-Validation Error for different # of trees
cv.err <- gbm(genre~., data=music.tr, distribution = 'gaussian', 
              n.tree = 500, cv.folds = 5)$cv.error
plot(cv.err, type='l', xlab="Number of trees", ylab="CV error")
# Plot the classification tree
tr1 <- boost$trees[[1]]
plot(tr1); text(tr1, pretty=TRUE, col='red', cex=.8)
boostmodel1 <- predict.boosting(boost, newdata = music.tst, type = 'class')
boostmodel<-boostmodel1$class
table_mat3 <- table(music.tst$genre, boostmodel)
table_mat3
```


#Support Vector Machine

```{r}
set.seed(1)
library(ISLR)
library(e1071)
library(ROCR)
s1 <- svm(as.factor(genre)~., data=music.tr, type='C', scale=TRUE)
st <- tune(svm, as.factor(genre)~., data=music.tr,
           ranges=list(cost=1:5, gamma=(1:5)/10))
st$best.model
summary(st$best.model)
p.svm <- predict(st$best.model)
table(music.tr$genre, p.svm, dnn=c("TRUE", "SVM"))
p.svm.tst <- predict(st$best.model, newdata = music.tst)
table(music.tst$genre, p.svm.tst, dnn=c("TRUE", "SVM"))
mean(music.tst$genre != p.svm.tst)
p.svm.tst2 <- predict(s1, newdata = music.tst)
table(music.tst$genre, p.svm.tst2, dnn=c("TRUE", "SVM"))
mean(music.tst$genre != p.svm.tst2)
library (ROCR)
#ROC Curve
s2 <- svm(genre~., data=music.tr, type='C', 
          scale=TRUE, decision.values = TRUE)
p.svm.tst2 <- attributes (predict(s2, newdata = music.tst, decision.values = T))$decision.values
pred <- prediction(p.svm.tst2, music.tst$genre)
perf <- performance(pred, "tpr", "fpr")
plot(perf, col='blue', cex.lab=1.5)
```


#Principle components
#clustering stuff

##SMOTE?

```{r}
library(DMwR)

## Smote : Synthetic Minority Oversampling Technique To Handle Class Imbalancy In Binary Classification
balanced.data <- SMOTE(genre ~., music, perc.over = 4800, k = 5, perc.under = 1000)

as.data.frame(table(balanced.data$Class))

```


